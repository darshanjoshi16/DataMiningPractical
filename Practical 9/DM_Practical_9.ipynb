{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DM_Practical_9.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyORnWL+wpUytVt49RQcQE+m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshanjoshi16/DataMiningPractical/blob/main/Practical%209/DM_Practical_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlZ3LPZZi8aK",
        "outputId": "770658da-5d86-4a1f-e93f-2bbf8dbcec04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Unnamed: 0': {0: 'Weak', 1: 'Strong', 2: 'Weak', 3: 'Weak', 4: 'Weak', 5: 'Strong', 6: 'Strong', 7: 'Weak', 8: 'Weak', 9: 'Weak', 10: 'Strong', 11: 'Strong', 12: 'Weak', 13: 'Strong'}}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "eps = np.finfo(float).eps\n",
        "\n",
        "def find_entropy(df):\n",
        "    target = df.keys()[-1]  # The last dataframe column is the target attribute (playGolf)\n",
        "    entropy = 0\n",
        "    values = df[target].unique()\n",
        "    # for each value in the target playGolf attribute values\n",
        "    for value in values:\n",
        "        # ratio of values occurring and entropy\n",
        "        fraction = df[target].value_counts()[value] / len(df[target])\n",
        "        entropy += -fraction * np.log2(fraction)\n",
        "    return entropy\n",
        "\n",
        "def find_entropy_attribute(df, attribute):\n",
        "    target = df.keys()[-1]\n",
        "    target_variables = df[target].unique()  \n",
        "    # unique values in target playGolf attribute (Yes, No)\n",
        "    variables = df[attribute].unique() # Identify Sunny, Overcast, Rainy\n",
        "    # attribute entropy\n",
        "    # Variables=[sunny, sunny....5, overcast1.....overcast4, Rainy1...Ra5 ]\n",
        "    entropy2 = 0\n",
        "    # for each attribute value in attribute values\n",
        "    for variable in variables:\n",
        "        \n",
        "        # value entropy\n",
        "        entropy = 0\n",
        "        # for each target value in target values (yes/no)\n",
        "        for target_variable in target_variables:\n",
        "            \n",
        "   # frequency of attribute and target values (boolean indexing, pandas dataframe filtering)\n",
        "          num = len(df[attribute][df[attribute] == variable][df[target] == target_variable])\n",
        "          den = len(df[attribute][df[attribute] == variable])\n",
        "          fraction = num / (den + eps)\n",
        "          entropy += -fraction * np.log2(fraction + eps)\n",
        "        fraction2 = den / len(df)\n",
        "        entropy2 += -fraction2 * entropy\n",
        "    return abs(entropy2)\n",
        "\n",
        "def bestClassifier(df):\n",
        "    # Entropy_att = []\n",
        "    # information gain array for all attributes\n",
        "    IG = []\n",
        "    # for all attributes excluding target\n",
        "    for key in df.keys()[:-1]:\n",
        "        # Entropy_att.append(find_entropy_attribute(df,key))\n",
        "        # calculate and record information gain value\n",
        "        IG.append(find_entropy(df) - find_entropy_attribute(df, key)) \n",
        "    return df.keys()[:-1][np.argmax(IG)]  \n",
        "\n",
        "def get_subtable(df, node, value):\n",
        "    return df[df[node] == value].reset_index(drop=True)\n",
        "\n",
        "def ID3split(df, tree=None):\n",
        "    target = df.keys()[-1]\n",
        "    \n",
        "    # Here we build our decision tree\n",
        "    # Get attribute with maximum information gain\n",
        "    node = bestClassifier(df) # 0.247\n",
        "    # Get distinct value of that attribute e.g Salary is node and Low,Med and High are values\n",
        "    attributeValues = np.unique(df[node])\n",
        "    # Create an empty dictionary to create tree (recursive-friendly definition)\n",
        "    if tree is None:               #  Outlook ->root node attribute\n",
        "        tree = {}\n",
        "        tree[node] = {}\n",
        "    # following loop recursively calls ID3split to create and add to the tree\n",
        "    # it runs till the tree is pure (leaf (result) node branches are added to the tree)\n",
        "    for value in attributeValues:\n",
        "        \n",
        "        # get the subtable from current node based on the value\n",
        "        subtable = get_subtable(df, node, value)\n",
        "        # get the most common target value in the subtable\n",
        "        targetValues, counts = np.unique(subtable[target], return_counts=True)\n",
        "        \n",
        "        # if the subtable is empty, assign the leaf node to the most common target value\n",
        "        if len(counts) == 1:\n",
        "            tree[node][value] = targetValues[0]\n",
        "        \n",
        "        else:\n",
        "            # recursively call ID3 to create subtrees\n",
        "            tree[node][value] = ID3split(subtable)  # Calling the function recursively\n",
        "    return tree\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/profthyagu/Python-Decision-Tree-Using-ID3/master/PlayTennis.csv\")\n",
        "decisionTree = ID3split(df)\n",
        "print(decisionTree)\n",
        "\n"
      ]
    }
  ]
}